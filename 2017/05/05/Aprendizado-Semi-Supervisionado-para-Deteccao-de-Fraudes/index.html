<!DOCTYPE html><html lang="pt,en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="O Laboratório de Aprendizado de Máquina em Finanças e Organizações (LAMFO) é um centro destinado a desenvolver, aplicar e estudar fenômenos organizacionais nas mais diversas áreas como Marketing, Logística, Administração Pública, Gestão de Pessoas e Finanças."><meta name="author" content="Pedro Albuquerque"><meta property="og:title" content="Aprendizado Semi-Supervisionado para Detecção de Fraudes"><meta property="og:description" content="O Laboratório de Aprendizado de Máquina em Finanças e Organizações (LAMFO) é um centro destinado a desenvolver, aplicar e estudar fenômenos organizacionais nas mais diversas áreas como Marketing, Logística, Administração Pública, Gestão de Pessoas e Finanças."><meta property="og:site_name" content="LAMFO"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><title>Aprendizado Semi-Supervisionado para Detecção de Fraudes - LAMFO</title><link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"><link rel="stylesheet" href="/css/style.css"><link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css"><!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]--><link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet"><script>!function(e,a,t,n,c,o,s){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,o=a.createElement(t),s=a.getElementsByTagName(t)[0],o.async=1,o.src="//www.google-analytics.com/analytics.js",s.parentNode.insertBefore(o,s)}(window,document,"script",0,"ga"),ga("create","UA-97417743-1","auto"),ga("send","pageview")</script><link rel="icon" href="/img/favicon.ico"></head><body><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class="container-fluid"><div class="navbar-header page-scroll"><button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button> <a class="navbar-brand" href="/">Inicio</a></div><div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1"><ul class="nav navbar-nav navbar-right"><li><a href="http://lamfo.unb.br/">Site</a></li><li><a href="/archives">Arquivos</a></li><li><a href="/data">Dados</a></li><li><a href="https://github.com/lamfo-unb">Github</a></li></ul></div></div></nav><header class="intro-header" style="background-image:url(/img/ferdinand-stohr-149422.jpg)"><div class="container"><div class="row"><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class="post-heading"><h1>Aprendizado Semi-Supervisionado para Detecção de Fraudes</h1><span class="meta">Posted by Matheus Facure on 2017-05-05</span></div></div></div></div></header><article><div class="container"><div class="row"><div class="col-lg-4 col-lg-offset-2 col-md-5 col-md-offset-1 post-tags"><a href="/tags/semi-supervisionado/">#semi-supervisionado</a> <a href="/tags/anomalia/">#anomalia</a> <a href="/tags/risco-de-credito/">#risco-de-credito</a></div><div class="col-lg-4 col-md-5 post-categories"></div><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><h1 id="Aprendizado-Semi-Supervisionado-para-Deteccao-de-Fraudes"><a href="#Aprendizado-Semi-Supervisionado-para-Deteccao-de-Fraudes" class="headerlink" title="Aprendizado Semi-Supervisionado para Detecção de Fraudes"></a>Aprendizado Semi-Supervisionado para Detecção de Fraudes</h1><p>Seja para detectar falhas em um avião ou usina nuclear, seja para perceber gastos ilícitos de um deputado ou seja para apontar sonegação no imposto de renda, a arte de notar comportamentos e padrões suspeitos pode ser bastante útil em diversos cenários. Pensando nisso, preparamos uma pequena lista de procedimentos para esse tipo de tarefa. Algumas delas serão incrivelmente simples e surpreendentemente efetivas. Outras, nem tão simples assim. De qualquer forma vamos nos concentrar em <strong>técnicas de aprendizado de máquina semi-supervisionado para detecção de anomalias</strong>. Não se preocupe se isso soou confuso. Antes de mais nada, vamos explicar tanto o que são anomalias quanto o que é aprendizado de máquina semi-supervisionado. Em seguida, vamos fornecer uma explicação intuitiva do funcionamento de cada uma das técnicas aqui expliradas, assim como esclarecer suas vantagens e desvantagens. Esse trabalho é livremente inspirado em uma pesquisa em que Chandola et al (2009) compilam várias técnicas de detecção de anomalia.</p><p>Essa pesquisa não pretende ser extensiva e nem rigorosa; nossa esperança é sermos o menos complicado possível na linguagem e o mais direto possível nas explicações intuitivas. Para uma versão mais detalhada e técnica, você pode conferir <a href="https://www.kaggle.com/matheusfacure/d/dalpozz/creditcardfraud/semi-supervised-anomaly-detection-survey" target="_blank" rel="external">implementação que fizemos diretamente no Kaggle</a> (em inglês).</p><h2 id="O-que-e-uma-anomalia"><a href="#O-que-e-uma-anomalia" class="headerlink" title="O que é uma anomalia?"></a>O que é uma anomalia?</h2><p>“Anomalias são padrões nos dados que não obedecem bem as noções definidas como normalidade” (Chandola et al, 2009). Em outras palavras, são dados que apresentam algum comportamento estranho, distinto do padrão normal. Por exemplo, os pontos $O_1$, $O_2$ na imagem abaixo estão isolados e fora das regiões de normalidade ($N_1$ e $N_2$), sendo assim considerados anomalias. Os pontos na região $O_3$, embora tenham uma vizinhança, também são anomalias, pois a região inteira está fora da de normalidade.</p><p><img src="/img/anomalia/anomaly.png" width="400"></p><p>Assim sendo, uma abordagem bastante direta para detecção de anomalias seria simplesmente definir a região onde os dados normais estão presentes e classificar tudo o que aparecer fora dessa região como anomalia. No entanto, isso é mais fácil de falar do que de fazer e há alguns desafios bastante complicados que surgem em problemas de detecção de anomalias:</p><ul><li><p>Modelar uma região que capture todas as noções de normalidade é extremamente difícil e as fronteiras entre normal e anormal geralmente não são bem definidas;</p></li><li><p>Anomalias podem ser o resultado de atividade maliciosa (e.g. fraudes bancárias). Nesse caso, há um adversário que está sempre tentando se adaptar para fazer com que as observações anômalas pareçam normais;</p></li><li><p>O que é normal pode mudar, isto é, uma noção de normalidade definida hoje pode não ser válida no futuro;</p></li><li><p>A noção de normalidade varia muito de aplicação para aplicação e não há um algoritmo geral o suficiente para capturá-la de forma ótima;</p></li><li><p>Conseguir amostrar o comportamento <strong>anormal</strong> é geralmente um dos maiores problemas em detecção de anomalias, sendo essas amostras extremamente escassas ou até mesmo inexistentes.</p></li></ul><p>Para lidar com esses problemas, nós sugerimos uma abordagem semi-supervisionada, que não requer nada além de uma pequena parcela de amostras anormais.</p><h2 id="O-que-e-aprendizado-de-maquina-semi-supervisionado"><a href="#O-que-e-aprendizado-de-maquina-semi-supervisionado" class="headerlink" title="O que é aprendizado de máquina semi-supervisionado"></a>O que é aprendizado de máquina semi-supervisionado</h2><p>Aprendizado de máquina é uma ciência que utiliza métodos da ciência da computação e estatística para analisar dados. As técnicas de aprendizado de máquina surgiram dentro do campo de inteligência artificial, como um meio de permitir que os computadores aprendessem uma forma de conhecimento própria. Hoje, aprendizado de máquina se expandiu em um campo autônomo e tem obtido sucessos também em problemas que demandam raciocínio estatístico além da capacidade humana. Dentro dos regimes de aprendizado de máquina, destaca-se o de aprendizado de máquina supervisionado, que foca em problemas de previsão: tendo uma base de dados com “alvos” para cada observação (pares $(x,y)$), a meta é aprender quais “alvos” ($y$) estão associados a quais dados ($x$). Isso é feito apresentando ao computador pares suficientes de dados e alvos, até que ele aprenda a associar um ao outro. Alguns exemplos desse tipo de problema são: identificar a presença de uma doença (alvo), dado os sintomas do paciente (dados); prever se o preço da ação de uma empresa vai subir ou cair (alvo), dado o histórico do mercado financeiro (dados); identificar de que pessoa é a face (alvo) em uma imagem (dados) ou classificar um livro (dados) em uma escola literária (alvo).</p><p>Uma limitação de aprendizado de máquina supervisionado é que pode ser extremamente custoso conseguir os alvos para cada observação (nós chamamos isso de nomear a base de dados). Por exemplo, considere o problema de prever o tema de um artigo de jornal (alvo) dado o seu conteúdo escrito (dados). Para que um computador faça isso, é preciso que antes alguém tenha lido uma grande quantidade (muitas vezes milhares) de artigos e nomeado cada um com um tema, para que assim o computador tenha acesso tanto ao conteúdo, quanto ao alvo que ele precisa prever, nesse caso o tema do artigo. Em cenários de detecção de anomalias, muitas vezes nós só temos dados que representam o caso normal, sendo extremamente difícil conseguir dados que representam anomalias. Em alguns extremos, como por exemplo o problema de detectar falhas em uma usina nuclear, conseguir exemplos de falhas críticas é tanto impossível quanto indesejável. Assim, com poucos ou nenhum exemplo do que é um caso anormal, o computador não tem informação suficiente para aprender os padrões estatísticos da anormalidade, tornando o problema de detectá-la extremamente difícil.</p><p>Uma possibilidade é utilizar <strong>aprendizado de máquina semi-supervisionado, em que consideramos apenas uma pequena parcela dos dados como estando nomeada</strong> e que os dados não nomeados só contêm exemplos do caso normal, que geralmente é abundante. Assim, nós utilizamos técnicas de aprendizado <strong>não</strong> supervisionado (para aprender a estrutura dos dados) nos dados não nomeados para extrair alguma noção de normalidade. Ao final dessa etapa não supervisionada, a máquina conseguirá associar cada observação com uma pontuação proporcional à probabilidade do dado ter vindo da região normal. Em seguida, usamos a parte nomeada dos dados (com exemplos tanto de anomalias, quanto do caso normal) para definir um limiar na pontuação, a partir do qual consideraremos a amostra como anômala.</p><p>OK… Talvez esse último parágrafo contém mais informação do que seja possível processas assim tão rapidamente. Se você não entendeu ainda não se preocupe, exemplos suficientes a seguir que tornarão isso mais claro. Vamos então ao estudo empírico, começando com os dados que usaremos.</p><h2 id="Os-dados"><a href="#Os-dados" class="headerlink" title="Os dados"></a>Os dados</h2><p>Nós vamos utilizar a base de dados <a href="https://www.kaggle.com/dalpozz/creditcardfraud" target="_blank" rel="external"><code>Credit Card Fraud Detection</code></a> para demonstrar o funcionamento de várias técnicas semi-supervisionadas de detecção de anomalia. Segundo a descrição providenciada pelo fornecedor, “a base de dados contém transações feitas por cartão de crédito em setembro de 2013 por consumidores europeus. Os dados são de transações que ocorreram no período de dois dias, contando com 284’807 transações, dentre as quais 492 são fraudes. A base é altamente desbalanceada; o alvo positivo (fraude) corresponde a apenas 0,172% de todas as transações. Todos os dados são de variáveis numéricas, que são o resultado de transformação PCA, além de variáveis sobre o tempo em que a transação foi efetivada e sobre a quantidade transacionada”. Para efeito de esclarecimento, a técnica de PCA permite manter a informação integral dos dados ao mesmo tempo que retira sua interpretabilidade. Nesse caso, a técnica foi utilizada para manter o sigilo das informações bancárias.</p><p>Nós repartimos os dados em 3 subamostras. Em primeiro lugar, reservamos 80% das amostras de transações normais para treino, o que totalizou 227’452 observações. Essa sub amostra será nossa base de treinamento. Em seguida, nós separamos o restante das observações normais e anormais igualmente, formando duas sub amostras com 28’677 observações cada, sendo 246 dessas observações anômalas. A primeira dessas sub amostrar será nosso set de validação e será utilizada para ajustar o limiar da pontuação de anomalia, que será produzida pelo modelo treinado na primeira etapa. Por fim, a última subamostra será para testar a performance da técnica considerada.</p><h2 id="Metricas-de-Avaliacao"><a href="#Metricas-de-Avaliacao" class="headerlink" title="Métricas de Avaliação"></a>Métricas de Avaliação</h2><p>Em problemas de previsão, a métrica de avaliação mais comum é a acurácia, que mede a proporção de acertos. No entanto, nesse cenário, como mais de 99% dos dados pertencem a uma única categoria, um preditor ingênuo prevendo todas as observações como sendo normais já conseguiria mais de 99% de acerto. Isso nos motiva a utilizar outras métricas: precisão e revocação. Intuitivamente, um sistema com alta revocação e precisão baixa apontaria muitas observações como sendo anômalas, conseguindo achar a maior parte das anomalias, mas a um custo muito alto em falsos positivos. Por outro lado, um sistema com alta precisão e baixa revocação apontaria poucos resultados como sendo transações fraudulentas, acertando-as, porém a um custo de várias fraudes passarem despercebidas. Assim, um sistema ideal teria que ponderar precisão e revocação em algum ponto ótimo. Formalmente, tempos precisão e revocação definidos como:</p><p>$$P=\frac{T_p}{T+p + F_p} \quad \quad \quad R=\frac{T_p}{T+p + F_n}$$</p><p>Em que $T_p$ são os verdadeiros positivos, $T_n$, os verdadeiros negativos, $F_p$ são os falsos positivos e $F_n$ são os falsos negativos.</p><p>Para que possamos comparar os modelos, é bom que tenhamos uma única métrica que resuma a performance. Para isso, vamos utilizar o $F_2$-score, que combina revocação e precisão, dando mais importância a primeira. Nós optamos por essa métrica porque acreditamos ser mais custoso não detectar uma fraude bancária que apontar uma transação como sendo fraudulenta e obter um alarme falso. Assim, quereríamos que nosso sistema coloque mais importância em conseguir boa revocação. Formalmente, temos:</p><p>$$F_2-score = (1+2)^2 \ast \frac{P \ast R}{2^2 \ast P + R}$$</p><h3 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h3><p>A maioria dos bons trabalhos utilizando essa base de dados fizeram uso de aprendizado puramente supervisionado com alguma técnica para lidar com o desbalanceamento severo entre as categorias. Particularmente interessante, é <a href="https://www.kaggle.com/joparga3/d/dalpozz/creditcardfraud/in-depth-skewed-data-classif-93-recall-acc-now" target="_blank" rel="external">este trabalho</a> disponível no Kaggle, que conseguiu uma precisão de 0,883, uma revocação de 0,619 e um $F_2$-score de 0,658. Como esse é o melhor resultado que achamos dentre os trabalhos disponíveis no Kaggle, vamos tomá-lo como benchmark.</p><h2 id="Tecnicas-de-Deteccao-de-Anomalias"><a href="#Tecnicas-de-Deteccao-de-Anomalias" class="headerlink" title="Técnicas de Detecção de Anomalias"></a>Técnicas de Detecção de Anomalias</h2><h3 id="Modelo-Gaussiano"><a href="#Modelo-Gaussiano" class="headerlink" title="Modelo Gaussiano"></a>Modelo Gaussiano</h3><p><img src="/img/anomalia/unigaussian.png" width="300"></p><p>A distribuição gaussiana é dada pela curva em formato de sino (acima) e é, com certeza, o modelo estatístico mais famoso, principalmente porque muitos fenômenos do dia a dia se encaixam muito bem nela. Intuitivamente, se um fenômeno tem comportamento gaussiano, podemos dizer que 95% das realizações desse fenômeno aconteceram a no máximo 2 desvios padrões de distância da média. Por exemplo, se a quantidade média transacionada por cartão de crédito for de R\$ 50,00, com um desvio padrão de R\$ 10,00, podemos dizer que 95% das transações serão de um valor entre 30 e 70 reais. Mais ainda, nós podemos dizer que transações fora desse intervalo são bastante improváveis, indicando que podem ser anomalias.<br>Aqui, nós estendemos a distribuição gaussiana para o seu caso multivariado, para assim capturar interações entre variáveis. Por exemplo, pode ser que uma grande quantia transacionada por si só não seja estranha, mas essa mesma quantia em uma especifica hora do dia, digamos de madrugada, seja. Nós podemos então modelar as margens da normalidade pela interação entre hora do dia e quantidade transacionada.</p><p><img src="/img/anomalia/multigaussian.png" width="350"></p><p>Nossos dados contêm 30 variáveis, logo não podemos visualizá-los em um gráfico como o acima, mas achar uma gaussiana de 30 dimensões é tão simples quanto achar uma de duas e é isso que fizemos com os nossos dados de treino, isto é, a subamostra não nomeada. Uma vez que tenhamos achado a gaussiana multidimensional que melhor se encaixa nos dados, nós podemos extrair a probabilidade de cada observação, que seria a altura da gaussiana multidimensional. Em seguida, nós usamos a subamostra (nomeada) de validação para ajustar um limiar a partir do qual consideraríamos uma transação como anômala. Apenas para efeito de clarificação, ao invés de usar a probabilidade para decidir o limiar, nós usamos o logaritmo da probabilidade (fizemos isso pois a probabilidade é um número muito pequeno que poderia dar problemas de precisão numérica, devido a capacidade limitada do computador em representar números com muitas casas decimais). Abaixo, vemos como as nossas três métricas de avaliação evoluem conforme mudamos o limiar.</p><p><img src="/img/anomalia/gaussiantuning.png" width="500"></p><p>O limiar ótimo escolhido foi de -269, isto é, amostrar com uma log probabilidade menor do que isso serão consideradas anomalias pelo nosso modelo. Com essa regra aprendida, nós tornamos a subamostra de teste para uma avaliação final. As pontuações nessa subamostra foram essas:</p><p>$$R=0,793 \quad P=0,701 \quad F_2=0,773$$</p><p>Apenas para que fique claro, uma revocação de 0,793 indica que, de todas as transações fraudulentas, nós conseguimos identificar 79,3% delas; uma precisão de 0,701 indica que, de todas as transações que previmos como sendo fraudulentas, 70,1% delas de fato o eram. O $F_2$-score não tem nenhuma interpretação intuitiva. Ele é apenas uma métrica que combina as outras duas de forma que possamos comparar os modelos mais facilmente. Por exemplo, podemos dizer que esse modelo é mais de 10 pontos melhor do que o nosso benchmark, que obteve um $F_2$-score de apenas 0,658. Abaixo está a matriz de confusão para mais detalhes sobre como o modelo classificou cada transação.</p><p><img src="/img/anomalia/confmatgaussian.png" width="450"></p><h3 id="Modelo-Histograma"><a href="#Modelo-Histograma" class="headerlink" title="Modelo Histograma"></a>Modelo Histograma</h3><p>Nem todas as variáveis dos nossos dados seguem uma distribuição gaussiana. Se plotarmos o histograma de algumas características, podemos ver isso claramente. A começar pela distribuição da variável que mede a quantidade transacionada, que tem uma distribuição bastante complicada, tanto para os dados normais quanto para os anormais. (Novamente, para efeito de esclarecimento, transformamos a variável de quantidade transacionada em sua forma logarítmica. Isso costuma deixar as distribuições mais balanceadas).</p><p><img src="/img/anomalia/hist1.png" width="400"><br><img src="/img/anomalia/hist2.png" width="400"><br><img src="/img/anomalia/hist3.png" width="400"><br><img src="/img/anomalia/hist4.png" width="400"></p><p>Quando vemos esses histogramas, podemos notar facilmente como algumas variáveis são ótimos indicadores de anomalias, uma vez que a distribuição (aproximada pelo histograma) dos dados anômalos e dos dados normais são bastante distinguíveis. Veja por exemplo a variável <code>V10</code>. A distribuição dos dados normais parece uma gaussiana bastante concentrada no zero, ao passo que os dados anormais parecem seguir uma distribuição bimodal, com um pico em aproximadamente -5 e outro em -15.</p><p>Nós vamos explorar essa diferença nos histogramas para construir uma pontuação de normalidade que será proporcional à probabilidade dos dados corresponderem a uma transação normal. Em primeiro lugar, vamos usar os dados de treinamento (não nomeados e com exemplos apenas dos casos normais) para estimar um histograma para cada variável nos dados. Assim, na hora de avaliar uma nova observação, nós vamos ver em qual dos compartimentos dos histogramas cada uma de suas variáveis cai. A pontuação de normalidade será então a média da altura desses compartimentos. É fácil perceber que se a observação for normal, ela cairá em uma região do histograma com compartimentos bastante altos.</p><p>Uma vez que tenhamos treinado esse modelo, vamos ajustar o limiar para essa nossa pontuação usando a subamostra de validação, que inclui dados nomeados, tanto do caso normal quanto do caso anormal.</p><p><img src="/img/anomalia/histtuning.png" width="400"></p><p>O melhor limiar, de acordo com o $F_2$-score, foi 36161. Com o modelo assim ajustado, conseguimos os seguintes resultados:</p><p>$$R=0,646 \quad P=0,170 \quad F_2=0,415$$</p><p>Podemos ver que esse modelo não é melhor que nosso benchmark, conseguindo um $F_2$-score de apenas 0,41. A performance do modelo é particularmente prejudicada pela baixa precisão, de 0,17. Isso indica que, de todas as observações que esse modelo classifica como sendo fraudulenta, apenas 17% delas de fato o são. Assim, o modelo incorre a um elevado custo em termos de falsos positivos. Devemos notar que esse modelo é capaz de aproximar qualquer distribuição por variável, mas não consegue capturar relações entre variáveis. Aparentemente, conseguir capturar interações é algo bastante importante nos dados aqui presente. Abaixo, na matriz de confusão, podemos ter uma ideia melhor do tipo de erro que estamos cometendo.</p><p><img src="/img/anomalia/confmathist.png" width="400"></p><h3 id="Modelo-de-Mistura-de-Gaussianas"><a href="#Modelo-de-Mistura-de-Gaussianas" class="headerlink" title="Modelo de Mistura de Gaussianas"></a>Modelo de Mistura de Gaussianas</h3><p><img src="/img/anomalia/gmm.png" width="300"></p><p>Anteriormente, no modelo gaussiano nós assumimos que os dados vinham de uma distribuição gaussiana multidimensional. Isso talvez seja um pouco restritivo. Como vimos nos histogramas acima, nem todas as variáveis seguem um modelo gaussiano. Nós podemos utilizar um modelo que relaxa essa hipótese, assumindo que os dados vêm de uma distribuição que é uma <strong>mistura de gaussianas</strong>. Pela imagem acima, podemos ver que essa hipótese não é nem um pouco restritiva e distribuição bastante complexas podem ser representadas por misturas de gaussianas. De fato, contanto que tenhamos gaussianas suficiente, qualquer distribuição poderá ser aproximada por uma mistura de gaussiana.</p><p>Se quisermos capturar interações entre variáveis, é fácil o suficiente estender o modelo para uma mistura de gaussianas multivariadas, como na imagem abaixo (dimensão da probabilidade/altura está suprimida e representada como curva de nível):</p><p><img src="/img/anomalia/gmmmultivariate.png" width="400"></p><p>No nosso modelo de detecção de anomalias, nós ajustamos 3 gaussianas aos dados com um algoritmo de Expectativa-Maximização. Fizemos isso utilizando apenas os dados de treino, que não estavam nomeados e que continham apenas casos de transações normais. Com esse modelo treinado, conseguimos mais uma vez ter acesso direto à probabilidade de cada amostra e mais uma vez optamos por utilizar o logaritmo da probabilidade para não ter que lidar com muitas casas decimais. Na subamostra de validação com dados nomeados, ajustamos um limiar para o logaritmo da probabilidade, para decidir abaixo de que pontuação consideraríamos uma anomalia. A evolução das três métricas conforme a evolução do limiar pode ser conferida abaixo.</p><p><img src="/img/anomalia/gmmtune.png" width="400"></p><p>O limiar ótimo escolhido foi de -101, de forma que observações com uma pontuação menor do que isso seriam consideradas anômalas. Por fim, tornamos à subamostra de teste para uma avaliação final. Os resultados foram os seguintes:</p><p>$$R=0,809 \quad P=0,726 \quad F_2=0,791$$</p><p>Esse foi o nosso melhor modelo, com elevada revocação e precisão. Para mais informações sobre o tipo de erro que esse modelo comete, abaixo colocamos a matriz de confusão da avaliação final, na subamostra de teste.</p><p><img src="/img/anomalia/confmatgmm.png" width="400"></p><h3 id="Modelo-de-Maquina-de-Suporte-Vetorial"><a href="#Modelo-de-Maquina-de-Suporte-Vetorial" class="headerlink" title="Modelo de Máquina de Suporte Vetorial"></a>Modelo de Máquina de Suporte Vetorial</h3><p>Máquinas de Suporte Vetoriais (MSV) são uma classe de modelos originalmente desenhadas para problemas supervisionados de categorização (ou classificação), em que o objetivo é separar dois tipos de observação. Intuitivamente, o que os MSVs fazem é achar o melhor plano de separação entre os dois tipos de dados. Isso pode parecer um pouco restritivo, já quem muitas vezes a superfície que separa duas classes de dados é composta por várias curvaturas, algo que um plano não conseguiria representar. Para superar essa limitação, as MSV fazer uso do truque do kernel, que representa os dados originais em um espaço dimensional maior, de forma que seja possível separar os dois tipos nesse novo espaço.</p><p><img src="/img/anomalia/kerneltrick.png" width="400"></p><p>Na imagem acima, vemos como o truque do kernel representa dados em altas dimensões de forma que eles possam ser separados por um plano. As MSVs não são os únicos modelos que podem se beneficiar do truque do kernel, mas elas são especialmente otimizadas para ele. Na verdade, o truque do kernel é tão computacionalmente ineficiente que se torna praticamente proibitivo de ser utilizado com qualquer técnica que não as máquinas de suporte vetorial.</p><p>Na versão não supervisionada da MSV, não estamos interessados em separar dois tipos de dados, pois os dados sequer são nomeados com os tipos. Em vez disso, queremos achar a melhor esfera que encapsule todos os dados. É fácil ver como isso é útil no problema de detecção de anomalias: a esfera que contiver os dados normais será definida como nossa fronteira de normalidade, de forma que dados fora dela serão classificados como anomalias. Mas e se os dados normais não se agruparem em uma esfera, mas em uma forma mais complicada? Como talvez, duas esferas ou um ovo? Isso também não é um problema para a versão não supervisionada das MSV, pois podemos usar o truque do kernel e representar os dados em uma dimensão maior até que eles estejam contidos em uma esfera</p><p><img src="/img/anomalia/oneclasssvm.png" width="400"></p><p>Na imagem abaixo, por exemplo, a região de normalidade é compreendida por duas esferas disjuntas, mas podemos ver como uma MSV consegue representar bem essas fronteiras. Na verdade, nós podemos dizer que o modelo não supervisionado de MSV é um <strong>aproximador universal de funções densidade de probabilidade</strong>, isto é, ele é capaz de aproximar qualquer distribuição possível. Uma severa desvantagem desse método é o fato dele ser extremamente ineficiente para treinar. Apenas para se ter uma ideia, dos modelos aqui avaliado, o segundo mais demorado de treinar é o de mistura de gaussianas, que leva aproximadamente 5min. O modelo MSV, por sua vez leva em torno de uma hora.</p><p>Após treinado, o modelo produz uma superfície de separação e nós podemos usar a distância entre uma observação e essa superfície como uma pontuação de anomalia para a observação. Distâncias negativas significam que a observação está fora esfera definida pela superfície. Assim, quanto mais negativa a pontuação, maior a probabilidade da amostra ser uma anomalia. Usando a subamostra de validação, nomeada com tipos normais e anômalos, nós ajustamos um limiar para essa pontuação.</p><p><img src="/img/anomalia/ocsvmtune.png" width="400"></p><p>O limiar escolhido foi de -22466.53 e com essa regra de decisão conseguimos a seguinte pontuação:</p><p>$$R=0,630 \quad P=0,323 \quad F_2=0,529$$</p><p>Podemos notar que esse modelo não bate o nosso benchmark, mesmo sendo capaz de representar qualquer distribuição possível. Isso acontece porque todo esse poder vem a um custo de muitas vezes encontrar superfícies de separação mais complicadas do que elas de fato deveriam ser. Algumas vezes - e provavelmente essa é uma delas - a complexidade adicional tem um custo mais elevado do que o benefício que se ganha com capacidade. Isso é o que chamamos de dilema viés-variância em aprendizado de máquina. Para mais informações sobre isso, temos também <a href="https://lamfo-unb.github.io/2017/04/29/Um-Olhar-Descontraido-Sobre-o-Dilema-Vies-Variancia/">esta excelente postagem</a> que cobre de maneira intuitiva o assunto.</p><p>Por fim, também é interessante notar como esse modelo peca mais em precisão, tendo uma revocação aceitável.</p><p><img src="/img/anomalia/conf-matSVM.png" width="400"></p><h3 id="Modelo-de-Floresta-de-Isolacao"><a href="#Modelo-de-Floresta-de-Isolacao" class="headerlink" title="Modelo de Floresta de Isolação"></a>Modelo de Floresta de Isolação</h3><p>O modelo de floresta de isolação é outro modelo que podemos considerar como um aproximador universal de distribuições. Assim, não precisamos colocar nenhuma hipótese restritiva na forma como os dados são distribuídos. Esse modelo é baseado em estruturas de árvores, uma classe de métodos de aprendizado de máquina que se popularizou muito nos últimos anos devido a sua simplicidade intuitiva e rapidez de treinamento. O modelo de floresta de isolação ajusta várias árvores de isolação aos dados. Para construir cada árvore de isolação, primeiro selecionamos aleatoriamente uma das variáveis nos dados. Em seguida, selecionamos um valor aleatório entre o máximo e o mínimo dessa variável, que será utilizado para separar os dados. Nós continuamos fazendo essas segmentações aleatórias até que todas as observações estejam isoladas, isto é, separadas das demais. Á primeira vista, pode parecer estranho como essas decisões aleatórias vão nos ajudar a detectar anomalias, mas eis a ideia. Na imagem abaixo, $X_1$ é uma observação normal e $X_0$ é uma observação anômala.</p><p><img src="/img/anomalia/isoforest.png" width="400"></p><p>Nossa esperança é que as anomalias sejam distintas dos dados normais, sendo que esses se aglomeram em algum local do espaço, enquanto que aquelas ficam mais isoladas. Assim, será preciso muito menos segmentações aleatórias para isolar uma anomalia do que para isolar uma observação normal. Nós podemos usar o número de segmentações até uma observação ser isolada como uma pontuação de anomalia: quanto maior esse número, menor a probabilidade da amostra ser uma anomalia. Quanto construímos várias dessas árvores e combinamos a pontuação de anomalia delas para conseguir uma pontuação final temos uma floresta de isolação. Após treinada, nós utilizamos a subamostra de validação para ajustar o limiar segundo essa pontuação.</p><p><img src="/img/anomalia/tuneforest.png" width="400"></p><p>Nosso limiar ótimo foi de 0,049. Com essa regra de decisão, tornamos à subamostra de teste para produzir uma avaliação final. Os resultados foram os seguintes:</p><p>$$R=0,760 \quad P=0,482 \quad F_2=0,681$$</p><p>Esses resultados batem o nosso benchmark, mas por pouco. Mais uma vez, podemos notar que o modelo tem sua performance puxada para baixo pela precisão, sendo que a revocação é bastante boa. Cabe ainda ressaltar que uma vantagem dessa metodologia frente as máquinas de suporte vetorial é a sua velocidade de treinamento e a capacidade de fácil paralelização. Como o treinamento de cada árvore é independente das demais, podemos realizá-lo em processos ou máquinas separadas, o que aumenta significativamente a rapidez com que possamos treinar uma floresta. Para finalizar, colocamos abaixo a matriz de confusão com respeito a avaliação final, na subamostra de teste.</p><p><img src="/img/anomalia/confmatforest.png" width="400"></p><h3 id="Modelo-Neural"><a href="#Modelo-Neural" class="headerlink" title="Modelo Neural"></a>Modelo Neural</h3><p>Nos últimos anos, redes neurais se tornaram o canivete suíço de aprendizado de máquina, tanto pela sua flexibilidade e efetividade na maioria dos cenários que envolvem problemas estatísticos de alta complexidade e não linearidade. Aqui, nós vamos utilizar um modelo neural chamado autocodificador, no qual pedimos que uma rede neural reconstrua o sinal que lhe é passado. No nosso caso, vamos pedir que ela reconstrua os dados da transação. Para impedir que a rede neural simplesmente copie o que lhe foi passado como reconstrução perfeita, nós colocamos uma camada de neurônios estreita no meio da rede neural. Essa cama terá apenas dois neurônios e como os nossos dados tem 30 variáveis, isso significa que a rede neural terá que aprender uma representação interna que condense 30 dimensões em apenas duas.</p><p><img src="/img/anomalia/autoencoder.png" width="400"></p><p>Nós vamos treinar um autocodificador utilizando apenas a subamostra de dados normais. Nossa esperança é que, ao não ser treinada para reconstruir as anomalias, a rede neural fará um péssimo trabalho reconstruindo-as, mas fará um ótimo trabalho reconstruindo observações normais. Assim, podemos usar o erro de reconstrução como uma pontuação para anormalidade: quanto maior o erro, maior a probabilidade da observação ser uma anomalia. Uma vez treinada a nossa rede neural, nós utilizamos a subamostra nomeada de validação para ajustar o limiar quanto à pontuação de anomalia.</p><p><img src="/img/anomalia/tuneautoenc.png" width="400"></p><p>O limiar ótimo foi um erro de 1.69, o que nos deu os seguintes resultados:</p><p>$$R=0,813 \quad P=0,294 \quad F_2=0,601$$</p><p>Podemos notar que esse modelo não é melhor que o nosso benchmark e, mais uma vez, a precisão prejudicou a performance do modelo. É preciso notar que o autocodificador não é otimizado para detectar anomalias, mas para reconstruir sinais. Nós suspeitamos que ele seja tão bom nisso que esteja reconstruindo bem tanto o sinal normal quanto anormal, tornando-o não muito bom para detecção de anomalia. Outro aspecto do autocodificador que merece atenção é que ele funciona como um método de redução de dimensões. Como utilizamos apenas dois neurônios na camada estreita, nós podemos guardar a atividade neural nessa camada como uma representação reduzida dos dados originais. Mais ainda, como temos apenas duas dimensões nessa versão reduzida, podemos colocá-la em um gráfico:</p><p><img src="/img/anomalia/autoencbiplot.png" width="450"></p><p>Acima, colocamos em branco os pontos normais e em preto, os anormais. É interessante perceber como a maioria das anomalias se agrupam em regiões distintas das regiões de normalidade. Talvez, o autocodificador não devesse ser utilizado como um método de detecção de anomalias em si, mas como um estágio de pré-processamento, onde primeiro conseguiríamos a representação acima e então usaríamos ela em conjunto com algum outro método de detecção de anomalia. Entretanto, isso ficará para um trabalho futuro.</p><p>Abaixo, colocamos a matriz de confusão do método neural para detalhar os tipos de erros que ele comete.</p><p><img src="/img/anomalia/confmatautoecn.png" width="400"></p><h2 id="Conclusao"><a href="#Conclusao" class="headerlink" title="Conclusão"></a>Conclusão</h2><p>Métodos semi-supervisionados de aprendizado de máquina são excelentes em cenários de detecção de anomalias em que há pouquíssimos exemplos do caso anômalo. Nessa base de dados em particular, até o momento desta escrita, não achamos nenhum projeto (no Kaggle) que constava resultados melhores do que os aqui encontrados. Nosso melhor modelo foi o de mistura de gaussianas, com um $F_2$-score de 79%, mas vale ressaltar que esse método não é universalmente superior. Em problemas de detecção de anomalias, estas são bastante específicas de acordo com o domínio de interesse, é improvável que um algoritmo seja sempre melhor do que outro.</p><p>Aqui, nosso contexto foi de detecção de fraudes, mas as técnicas foram desenvolvidas de forma bastante geral para se estenderem a outras aplicações. Algumas delas que sugerimos são detecção de carteis, irregularidades em leilões, inadimplência, sonegação e corrupção.</p><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f0d5c0f023693e"></script><div class="addthis_sharing_toolbox"></div></div><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><hr><h3>Comentários:</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></div></div></div></article><hr><footer><div class="container"><div class="row"><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href="https://www.facebook.com/lamfounb/" target="_blank"><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i> <i class="fa fa-facebook fa-stack-1x fa-inverse"></i></span></a></li><li><a href="https://github.com/lamfo-unb" target="_blank"><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i> <i class="fa fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href="mailto:lamfo/(at)unb/(dot)br" target="_blank"><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i> <i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">&copy; 2017 Pedro Albuquerque<br></p><p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p><p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p></div></div></div></footer><script src="//code.jquery.com/jquery-2.1.4.min.js"></script><script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script><script type="text/javascript">var disqus_shortname="lamfo";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe"></script></body></html>